# ğŸ“ Complete Parameter Calculation Guide for nanochat

## ğŸ“– Quick Summary

For the **d20 speedrun model**:

```
Total: 561M parameters
â”œâ”€ Token Embedding (wte):     84M  (15%)
â”œâ”€ 20 Ã— Transformer Blocks:  393M  (70%)
â”‚   â”œâ”€ Attention per block:  6.6M
â”‚   â””â”€ MLP per block:       13.1M
â””â”€ LM Head (unembedding):     84M  (15%)
```

---

## ğŸ§® The Master Formula

For any nanochat model with depth `d`, vocab `V`, model_dim `D`:

```python
# Dimensions
model_dim = d Ã— 64
num_heads = ceil(model_dim / 128)
vocab_size = 65,536  # 2^16

# Components
token_embedding = V Ã— D
per_block = 12 Ã— DÂ²  # (4Ã—DÂ² attention + 8Ã—DÂ² MLP)
all_blocks = d Ã— (12 Ã— DÂ²)
lm_head = D Ã— V

# Total
TOTAL = 2Ã—(VÃ—D) + dÃ—(12Ã—DÂ²)
```

---

## ğŸ” Step-by-Step Calculation for d20

### Step 1: Determine model dimensions

```python
depth = 20
model_dim = 20 Ã— 64 = 1,280
num_heads = ceil(1,280 / 128) = 10
head_dim = 1,280 / 10 = 128
vocab_size = 65,536
```

### Step 2: Token Embedding

```python
wte = nn.Embedding(65536, 1280)
params = 65,536 Ã— 1,280 = 83,886,080
```

### Step 3: Single Transformer Block

#### Attention (4 matrices):
```python
# Q, K, V, Output projections
each_projection = 1,280 Ã— 1,280 = 1,638,400
total_attention = 4 Ã— 1,638,400 = 6,553,600
```

**Why 4 matrices?**
- Q: `Linear(1280 â†’ 1280)` = 1,638,400
- K: `Linear(1280 â†’ 1280)` = 1,638,400
- V: `Linear(1280 â†’ 1280)` = 1,638,400
- Output: `Linear(1280 â†’ 1280)` = 1,638,400

#### MLP (2 matrices with 4Ã— expansion):
```python
# First: expand 4Ã—
first = 1,280 Ã— (4 Ã— 1,280) = 1,280 Ã— 5,120 = 6,553,600

# Activation: 0 params

# Second: project back
second = (4 Ã— 1,280) Ã— 1,280 = 5,120 Ã— 1,280 = 6,553,600

total_mlp = 6,553,600 + 6,553,600 = 13,107,200
```

#### RMSNorm: 0 params (functional, no learnable weights)

#### Total per block:
```python
block = attention + mlp + rmsnorm
      = 6,553,600 + 13,107,200 + 0
      = 19,660,800
```

### Step 4: All 20 blocks

```python
all_blocks = 20 Ã— 19,660,800 = 393,216,000
```

### Step 5: LM Head

```python
lm_head = nn.Linear(1280, 65536)
params = 1,280 Ã— 65,536 = 83,886,080
```

### Step 6: Total

```python
TOTAL = token_emb + all_blocks + lm_head
      = 83,886,080 + 393,216,000 + 83,886,080
      = 560,988,160
      â‰ˆ 561M
```

---

## ğŸ’¡ Key Insights

### 1. **MLP dominates each block** (2:1 ratio)
```
Attention: 4Ã—dÂ²  = 4 Ã— 1,280Â² = 6.6M
MLP:       8Ã—dÂ²  = 8 Ã— 1,280Â² = 13.1M
Ratio:     1:2
```

Why? MLP expands 4Ã— then projects back = `2 Ã— (d Ã— 4d) = 8dÂ²`

### 2. **Embeddings are symmetric** (but untied!)
```
Token embedding = vocab Ã— d_model = 84M
LM head         = d_model Ã— vocab = 84M
```

They're the same size (transposes), but **separate weights**.

### 3. **Layers scale quadratically, embeddings linearly**
```
Double d_model:  4Ã— more params per layer! (dÂ² â†’ (2d)Â² = 4dÂ²)
Double vocab:    2Ã— more embedding params  (v â†’ 2v)
```

This is why wide models are expensive!

### 4. **No "hidden" parameters**

These have **0 learnable parameters**:
- RMSNorm (functional)
- RoPE (precomputed lookup)
- Activations (ReLUÂ²)
- Attention mechanism itself (uses QKV projections)

### 5. **Optimization split**

```
Muon optimizer:   393M (70%) - all transformer blocks
AdamW optimizer:  168M (30%) - embeddings + lm_head
```

This is why untied embeddings matter - different optimization!

---

## ğŸ“Š Comparison: How Size Scales

| Depth | d_model | Block | Layers | Emb+LM | Total | Cost |
|-------|---------|-------|--------|--------|-------|------|
| d4    | 256     | 0.8M  | 3M     | 34M    | 37M   | <$1  |
| d8    | 512     | 3.1M  | 25M    | 67M    | 92M   | ~$5  |
| d12   | 768     | 7.1M  | 85M    | 101M   | 186M  | ~$30 |
| **d20**| **1280** | **19.7M** | **393M** | **168M** | **561M** | **~$100** |
| d26   | 1664    | 33.2M | 863M   | 218M   | 1.08B | ~$300|
| d32   | 2048    | 50.3M | 1.61B  | 268M   | 1.88B | ~$800|

**Observation:** 
- d4 â†’ d8: 2Ã— depth, 4Ã— d_model â†’ 2.5Ã— params
- d20 â†’ d26: 1.3Ã— depth, 1.3Ã— d_model â†’ 1.9Ã— params

It's more efficient to go deeper than wider!

---

## ğŸ¯ Practice Examples

### Example 1: What if we used MQA?

**Question:** d20 model with `n_kv_head=1` (Multi-Query Attention)

**Answer:**
```python
# Attention changes:
Q: 1,280 Ã— 1,280 = 1,638,400  (unchanged)
K: 1,280 Ã— 128   = 163,840    (10Ã— smaller!)
V: 1,280 Ã— 128   = 163,840    (10Ã— smaller!)
Output: 1,280 Ã— 1,280 = 1,638,400  (unchanged)

New attention = 1,638,400 + 163,840 + 163,840 + 1,638,400
              = 3,604,480  (was 6,553,600)

Saved per block = 6.6M - 3.6M = 3M
Saved total = 20 Ã— 3M = 60M

New total = 561M - 60M = 501M

Reduction: 10.7%
```

**Takeaway:** MQA saves 10% params, but HUGE KV cache savings for inference!

---

### Example 2: What if vocab was 256K (like GPT-4)?

**Question:** Same d20, but `vocab_size=256K`

**Answer:**
```python
Old embeddings = 2 Ã— (65K Ã— 1,280) = 168M
New embeddings = 2 Ã— (256K Ã— 1,280) = 655M

Increase = 655M - 168M = 487M

New total = 561M + 487M = 1,048M â‰ˆ 1.05B

Parameters increased by 87%!
```

**Takeaway:** Large vocab is expensive! That's why 65K is sweet spot.

---

### Example 3: Calculate params for d16

**Question:** What's the parameter count for d16?

**Solution:**
```python
depth = 16
model_dim = 16 Ã— 64 = 1,024
num_heads = ceil(1,024 / 128) = 8
vocab = 65,536

# Embeddings
emb = 2 Ã— (65,536 Ã— 1,024) = 134,217,728

# Per block
block = 12 Ã— 1,024Â² = 12 Ã— 1,048,576 = 12,582,912

# All blocks
blocks = 16 Ã— 12,582,912 = 201,326,592

# Total
total = 134,217,728 + 201,326,592 = 335,544,320
      â‰ˆ 336M params
```

---

## ğŸ§ª DIY Calculator

Try this Python function:

```python
def count_params(depth, vocab=65536):
    """Calculate nanochat parameters for any depth."""
    d = depth * 64  # model_dim
    emb = 2 * vocab * d
    blocks = depth * 12 * d * d
    return emb + blocks

# Examples
print(f"d10:  {count_params(10)/1e6:.0f}M")
print(f"d15:  {count_params(15)/1e6:.0f}M")
print(f"d20:  {count_params(20)/1e6:.0f}M")  # 561M
print(f"d25:  {count_params(25)/1e6:.0f}M")
print(f"d30:  {count_params(30)/1e6:.0f}M")
```

---

## ğŸ“ Why This Matters

### For Understanding:
- Know where parameters live (70% in transformers!)
- Understand memory requirements (param count Ã— bytes per param)
- Debug shape mismatches in code

### For Research:
- Design efficient architectures (MQA, smaller vocab, etc.)
- Calculate compute budgets (FLOPs âˆ 6 Ã— params Ã— tokens)
- Apply scaling laws (bigger vs wider vs more data)

### For Implementation:
- Verify model loaded correctly
- Estimate training time/cost
- Plan inference optimization (quantization targets)

---

## ğŸ“š References

Scripts to check out:
1. `calculate_params.py` - Detailed step-by-step calculation
2. `visualize_params.py` - ASCII art diagrams
3. `manual_param_calculation.md` - This guide with examples

Model files:
- `nanochat/gpt.py` - See the actual architecture
- `scripts/base_train.py` - See how dimensions are derived

---

## âœ… Quick Self-Test

**Can you answer these?**

1. How many parameters in the Q projection for d20?
   <details><summary>Answer</summary>1,280 Ã— 1,280 = 1,638,400</details>

2. What's the attention:MLP ratio?
   <details><summary>Answer</summary>1:2 (4Ã—dÂ² vs 8Ã—dÂ²)</details>

3. Why are embeddings 30% of total params?
   <details><summary>Answer</summary>Large vocab (65K) + untied weights doubles embedding cost</details>

4. How many params if we used GQA with 2 KV heads?
   <details><summary>Answer</summary>~535M (saves ~26M from attention)</details>

5. What has 0 parameters?
   <details><summary>Answer</summary>RMSNorm, RoPE, activations, attention mechanism</details>

---

**You now know how to calculate transformer parameters by hand!** ğŸ‰

Try calculating for different configurations and see what you get!
