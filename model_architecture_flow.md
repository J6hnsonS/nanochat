# ğŸ—ï¸ Exact Architecture Flow of nanochat d20 Model

## ğŸ“‹ Quick Answer

Your proposed structure is **almost correct**, but here's the **precise** flow:

```
Input tokens (B, T)
  â†“
Token Embedding (wte)
  â†“
RMSNorm
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  20Ã— Transformer Blocks:        â”‚
â”‚                                 â”‚
â”‚  For each block:                â”‚
â”‚    x_input (residual saved)     â”‚
â”‚      â†“                           â”‚
â”‚    RMSNorm                       â”‚
â”‚      â†“                           â”‚
â”‚    Attention (Q,K,V + RoPE)     â”‚
â”‚      â†“                           â”‚
â”‚    x = x_input + attn_out       â”‚  â† Residual Add
â”‚      â†“                           â”‚
â”‚    x_input2 (residual saved)    â”‚
â”‚      â†“                           â”‚
â”‚    RMSNorm                       â”‚
â”‚      â†“                           â”‚
â”‚    MLP (Linearâ†’ReLUÂ²â†’Linear)    â”‚
â”‚      â†“                           â”‚
â”‚    x = x_input2 + mlp_out       â”‚  â† Residual Add
â”‚      â†“                           â”‚
â”‚  (repeat 20 times)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Final RMSNorm
  â†“
LM Head (Linear)
  â†“
Logit Softcapping (tanh)
  â†“
Output logits (B, T, vocab_size)
  â†“
Cross-entropy Loss
```

---

## ğŸ” Detailed Code-Level Architecture

Let me trace through the **actual code** to show you EXACTLY what happens:

### 1ï¸âƒ£ Token Embedding + Initial Norm

```python
# From gpt.py lines 256-257
x = self.transformer.wte(idx)        # (B, T) â†’ (B, T, 1280)
x = norm(x)                           # RMSNorm right after embedding
```

**Shape:** `(batch, seq_len) â†’ (batch, seq_len, 1280)`

---

### 2ï¸âƒ£ Single Transformer Block (repeated 20Ã—)

```python
# From gpt.py lines 132-135 (Block.forward)
def forward(self, x, cos_sin, kv_cache):
    x = x + self.attn(norm(x), cos_sin, kv_cache)  # Pre-norm + residual
    x = x + self.mlp(norm(x))                      # Pre-norm + residual
    return x
```

Let me break this down step-by-step:

#### Step A: Attention Sub-layer
```python
# Pseudo-code expansion:
x_input = x                    # Save for residual
x_normed = norm(x_input)       # RMSNorm (pre-norm)
attn_out = attention(x_normed) # Attention with RoPE inside
x = x_input + attn_out         # Residual connection
```

**Inside Attention:**
```python
# From gpt.py lines 66-110 (CausalSelfAttention.forward)
1. Project to Q, K, V
2. Reshape to multi-head format
3. Apply RoPE to Q and K
4. Apply QK normalization
5. Scaled dot-product attention
6. Concatenate heads
7. Output projection
```

#### Step B: MLP Sub-layer
```python
# Pseudo-code expansion:
x_input = x                    # Save for residual (new residual!)
x_normed = norm(x_input)       # RMSNorm (pre-norm)
mlp_out = mlp(x_normed)        # MLP with ReLUÂ²
x = x_input + mlp_out          # Residual connection
```

**Inside MLP:**
```python
# From gpt.py lines 119-123
x = self.c_fc(x)          # Linear: 1280 â†’ 5120
x = F.relu(x).square()    # ReLUÂ² activation
x = self.c_proj(x)        # Linear: 5120 â†’ 1280
```

---

### 3ï¸âƒ£ Final Processing

```python
# From gpt.py lines 258-276
# After all 20 blocks:
for block in self.transformer.h:
    x = block(x, cos_sin, kv_cache)
    
x = norm(x)                           # Final RMSNorm

# Forward the lm_head (compute logits)
softcap = 15
if targets is not None:
    logits = self.lm_head(x)          # (B, T, 1280) â†’ (B, T, 65536)
    logits = softcap * torch.tanh(logits / softcap)  # Logits softcapping
    logits = logits.float()           # Use fp32 for numerical stability
    loss = F.cross_entropy(...)       # Cross-entropy loss
    return loss
```

---

## ğŸ¯ Your Original Structure vs Actual

### âŒ Your Version (close but not quite):
```
Embedding â†’ 20Ã—(
  Attn layer(RMSnorm â†’ RoPE for Q,K â†’ Multihead attn) â†’
  MLP layer(RMSnorm â†’ MLP â†’ ReLU)
) â†’ LM head â†’ softcap â†’ Muon & Adam
```

### âœ… Actual Structure:
```
Embedding â†’ RMSNorm â†’ 20Ã—(
  RMSNorm â†’ Attn(RoPE inside) â†’ Add Residual â†’
  RMSNorm â†’ MLP(ReLUÂ² inside) â†’ Add Residual
) â†’ RMSNorm â†’ LM head â†’ Softcap â†’ Loss
```

### ğŸ”‘ Key Differences:

1. **RMSNorm BEFORE each sublayer** (pre-norm), not after
2. **Residual connections AFTER each sublayer** (x = x + sublayer(norm(x)))
3. **Initial RMSNorm** right after token embedding
4. **Final RMSNorm** before LM head
5. **RoPE is applied INSIDE attention** (not a separate layer)
6. **ReLUÂ² is INSIDE the MLP** (not after)
7. **Softcapping is part of forward pass**, not a separate layer
8. **Muon/AdamW are optimizers** (not part of model structure)

---

## ğŸ“ Visual ASCII Architecture

```
                    INPUT: Token IDs [B, T]
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Token Embedding (wte)               â”‚
        â”‚     [B, T] â†’ [B, T, 1280]              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          RMSNorm (initial)              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘                                           â•‘
        â•‘    TRANSFORMER BLOCK 0                    â•‘
        â•‘                                           â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  x_saved â† x  (save residual) â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚       RMSNorm(x)              â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  Project Q, K, V              â”‚     â•‘
        â•‘    â”‚  Q: [B,T,1280] â†’ [B,10,T,128]â”‚     â•‘
        â•‘    â”‚  K: [B,T,1280] â†’ [B,10,T,128]â”‚     â•‘
        â•‘    â”‚  V: [B,T,1280] â†’ [B,10,T,128]â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  Apply RoPE to Q, K           â”‚     â•‘
        â•‘    â”‚  (rotary position encoding)   â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  QK Normalization             â”‚     â•‘
        â•‘    â”‚  Q â† norm(Q), K â† norm(K)     â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  Scaled Dot-Product Attention â”‚     â•‘
        â•‘    â”‚  softmax(QK^T/âˆš128) Ã— V      â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  Concatenate & Project        â”‚     â•‘
        â•‘    â”‚  [B,10,T,128] â†’ [B,T,1280]   â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  x â† x_saved + attn_output    â”‚     â•‘
        â•‘    â”‚  (RESIDUAL CONNECTION)        â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘    â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”‚â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ       â•‘
        â•‘                  â”‚                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  x_saved â† x  (save residual) â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚       RMSNorm(x)              â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  Linear (expand 4Ã—)           â”‚     â•‘
        â•‘    â”‚  [B,T,1280] â†’ [B,T,5120]     â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  ReLUÂ² Activation             â”‚     â•‘
        â•‘    â”‚  x â† relu(x).square()         â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  Linear (project back)        â”‚     â•‘
        â•‘    â”‚  [B,T,5120] â†’ [B,T,1280]     â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•‘                  â–¼                        â•‘
        â•‘    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
        â•‘    â”‚  x â† x_saved + mlp_output     â”‚     â•‘
        â•‘    â”‚  (RESIDUAL CONNECTION)        â”‚     â•‘
        â•‘    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
        â•‘                  â”‚                        â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              â”‚
                              â–¼
                    (Repeat 19 more times)
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Final RMSNorm                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         LM Head (Linear)                â”‚
        â”‚    [B,T,1280] â†’ [B,T,65536]            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Logit Softcapping                  â”‚
        â”‚   logits â† 15*tanh(logits/15)          â”‚
        â”‚   (bounds logits to [-15, 15])         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Convert to Float32                 â”‚
        â”‚   (for numerical stability)             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                  OUTPUT: Logits [B, T, 65536]
                              â”‚
                              â–¼
                    (If training: compute loss)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Cross-Entropy Loss                 â”‚
        â”‚   F.cross_entropy(logits, targets)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Code Trace Through One Forward Pass

Let's trace a concrete example with batch_size=2, seq_len=4:

```python
# Input
idx = [[1, 2, 3, 4],
       [5, 6, 7, 8]]  # shape: (2, 4)

# Step 1: Token Embedding
x = transformer.wte(idx)
# shape: (2, 4, 1280)

# Step 2: Initial norm
x = norm(x)
# shape: (2, 4, 1280)

# Step 3-22: 20 Transformer blocks
for block_idx in range(20):
    # ---- Attention sub-layer ----
    x_residual = x                    # Save for residual
    x_normed = norm(x)                # Pre-norm
    
    # Inside attention:
    q = c_q(x_normed)                 # (2,4,1280) â†’ (2,4,1280)
    k = c_k(x_normed)                 # (2,4,1280) â†’ (2,4,1280)
    v = c_v(x_normed)                 # (2,4,1280) â†’ (2,4,1280)
    
    q = q.view(2, 4, 10, 128)         # Reshape to heads
    k = k.view(2, 4, 10, 128)
    v = v.view(2, 4, 10, 128)
    
    q = apply_rotary_emb(q, cos, sin) # Apply RoPE
    k = apply_rotary_emb(k, cos, sin)
    
    q = norm(q)                       # QK norm
    k = norm(k)
    
    q = q.transpose(1, 2)             # (2,10,4,128)
    k = k.transpose(1, 2)
    v = v.transpose(1, 2)
    
    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)
    y = y.transpose(1, 2)             # (2,4,10,128)
    y = y.reshape(2, 4, 1280)         # Concatenate heads
    
    attn_out = c_proj(y)              # Output projection
    
    x = x_residual + attn_out         # RESIDUAL ADD
    
    # ---- MLP sub-layer ----
    x_residual = x                    # Save for residual
    x_normed = norm(x)                # Pre-norm
    
    # Inside MLP:
    h = c_fc(x_normed)                # (2,4,1280) â†’ (2,4,5120)
    h = F.relu(h).square()            # ReLUÂ² activation
    mlp_out = c_proj(h)               # (2,4,5120) â†’ (2,4,1280)
    
    x = x_residual + mlp_out          # RESIDUAL ADD
    
# shape after 20 blocks: (2, 4, 1280)

# Step 23: Final norm
x = norm(x)
# shape: (2, 4, 1280)

# Step 24: LM head
logits = lm_head(x)
# shape: (2, 4, 65536)

# Step 25: Softcapping
softcap = 15
logits = softcap * torch.tanh(logits / softcap)
# shape: (2, 4, 65536), values in [-15, 15]

# Step 26: Convert to float32
logits = logits.float()

# Step 27: Compute loss (if training)
if targets is not None:
    loss = F.cross_entropy(logits.view(-1, 65536), targets.view(-1))
    return loss
else:
    return logits
```

---

## ğŸ¯ Key Architectural Details

### 1. **Pre-Norm Architecture**
```python
# PRE-NORM (what nanochat uses):
x = x + sublayer(norm(x))

# vs POST-NORM (older, like original Transformer):
x = norm(x + sublayer(x))
```

**Why pre-norm?**
- Better gradient flow
- More stable training
- Can train deeper models

### 2. **Two Separate Residual Streams**
```python
# Block has TWO residual connections:
x = x + attention(norm(x))    # First residual
x = x + mlp(norm(x))          # Second residual (NOT from original input!)
```

Each sub-layer has its own residual, they don't share!

### 3. **RMSNorm Placement**
```
- After token embedding (stabilize magnitudes)
- Before EACH attention layer
- Before EACH MLP layer  
- After final transformer block (before logits)
```

Total: `1 + 20Ã—2 + 1 = 42 RMSNorm calls` (but 0 parameters!)

### 4. **RoPE is Inside Attention**

RoPE is not a "layer" - it's an operation inside attention:
```python
q = self.c_q(x)               # Project to Q
q = apply_rotary_emb(q, ...)  # Apply RoPE IN-PLACE
# Q now has positional information!
```

### 5. **Softcapping Happens in Forward**

Not a separate layer, just a transformation:
```python
logits = lm_head(x)
logits = 15 * tanh(logits / 15)  # Bounds to [-15, 15]
```

---

## ğŸ§ª Verify This Yourself

Run this to see the actual structure:

```python
import torch
from nanochat.gpt import GPT, GPTConfig

# Create d20 model
config = GPTConfig(
    sequence_len=2048,
    vocab_size=65536,
    n_layer=20,
    n_head=10,
    n_kv_head=10,
    n_embd=1280
)

model = GPT(config)

# Print architecture
print(model)

# Trace a forward pass
dummy_input = torch.randint(0, 65536, (2, 4))
with torch.no_grad():
    output = model(dummy_input)
    print(f"Input shape:  {dummy_input.shape}")
    print(f"Output shape: {output.shape}")  # Should be (2, 4, 65536)
```

---

## ğŸ“ Summary: Correct Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input tokens [B, T]                                     â”‚
â”‚   â†“                                                     â”‚
â”‚ Token Embedding â†’ [B, T, 1280]                         â”‚
â”‚   â†“                                                     â”‚
â”‚ RMSNorm                                                 â”‚
â”‚   â†“                                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ FOR i = 0 to 19:                                â”‚   â”‚
â”‚ â”‚   x â† x + Attention(RMSNorm(x))  [with RoPE]    â”‚   â”‚
â”‚ â”‚   x â† x + MLP(RMSNorm(x))        [with ReLUÂ²]   â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   â†“                                                     â”‚
â”‚ RMSNorm                                                 â”‚
â”‚   â†“                                                     â”‚
â”‚ LM Head â†’ [B, T, 65536]                                â”‚
â”‚   â†“                                                     â”‚
â”‚ Logit Softcapping (15*tanh(Â·/15))                      â”‚
â”‚   â†“                                                     â”‚
â”‚ Float32 conversion                                      â”‚
â”‚   â†“                                                     â”‚
â”‚ Cross-Entropy Loss (if training)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimizers (separate from architecture):**
- Muon: for all Linear layers in transformer blocks (70% of params)
- AdamW: for token embedding + LM head (30% of params)

---

Hope this clarifies the exact structure! The key insight is the **pre-norm + residual** pattern repeated 20 times. ğŸ¯
