# Tokenizer Quick Reference

## ğŸ¯ The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              nanochat Tokenization Strategy                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  PROBLEM: Need both training AND fast inference             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   tiktoken     â”‚         â”‚  HuggingFace   â”‚             â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
â”‚  â”‚ âœ… Fast        â”‚         â”‚ âœ… Training    â”‚             â”‚
â”‚  â”‚ âŒ No training â”‚         â”‚ âŒ Bloated     â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                              â”‚
â”‚  SOLUTION: Use BOTH (the best parts)                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    rustbpe           tiktoken                    â”‚       â”‚
â”‚  â”‚    (training)   â”€â”€â”€â–º (inference)                 â”‚       â”‚
â”‚  â”‚                                                   â”‚       â”‚
â”‚  â”‚    Custom Rust       OpenAI's proven             â”‚       â”‚
â”‚  â”‚    ~500 lines        Battle-tested               â”‚       â”‚
â”‚  â”‚    Fast training     Blazing fast                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Performance Cheat Sheet

### Training Speed (10MB text)
```
minbpe (Python):     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  300s
HuggingFace:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         15s
rustbpe:             â–ˆâ–ˆâ–ˆ                             8s  â† 2x faster!
```

### Inference Speed (1M tokens)
```
Python:              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   30s
HuggingFace:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          1.2s
tiktoken:            â–ˆ                              0.3s  â† 4x faster!
```

### Code Complexity
```
HuggingFace:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  20,000 lines
rustbpe:             â–ˆâ–ˆ                              ~500 lines  â† 40x simpler!
```

## ğŸ”„ The Workflow

```
Step 1: TRAIN (happens once)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python -m scripts.tok_train            â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Stream data â†’ rustbpe           â”‚   â”‚
â”‚  â”‚                                  â”‚   â”‚
â”‚  â”‚ for doc in dataset:              â”‚   â”‚
â”‚  â”‚     chunks = regex.split(doc)    â”‚   â”‚
â”‚  â”‚     count_pairs(chunks)          â”‚   â”‚  â† Parallel (rayon)
â”‚  â”‚     merge_best_pair()            â”‚   â”‚
â”‚  â”‚                                  â”‚   â”‚
â”‚  â”‚ Repeat vocab_size - 256 times   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                         â”‚
â”‚  Output: merges dict {(a,b): c}        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
Step 2: EXPORT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Export to tiktoken format              â”‚
â”‚                                         â”‚
â”‚  pattern = rustbpe.get_pattern()        â”‚
â”‚  merges = rustbpe.get_mergeable_ranks() â”‚
â”‚                                         â”‚
â”‚  enc = tiktoken.Encoding(               â”‚
â”‚      name="nanochat",                   â”‚
â”‚      pat_str=pattern,                   â”‚
â”‚      mergeable_ranks=merges,            â”‚
â”‚      special_tokens={...}               â”‚
â”‚  )                                      â”‚
â”‚                                         â”‚
â”‚  pickle.dump(enc, "tokenizer.pkl")      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
Step 3: USE (happens billions of times)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load once, use everywhere              â”‚
â”‚                                         â”‚
â”‚  tokenizer = get_tokenizer()            â”‚
â”‚                                         â”‚
â”‚  # Training                             â”‚
â”‚  ids = tokenizer.encode(text)           â”‚  â† tiktoken (fast!)
â”‚  loss = model(ids)                      â”‚
â”‚                                         â”‚
â”‚  # Inference                            â”‚
â”‚  tokens = tokenizer.encode(prompt)      â”‚  â† tiktoken (fast!)
â”‚  output = model.generate(tokens)        â”‚
â”‚  text = tokenizer.decode(output)        â”‚  â† tiktoken (fast!)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¨ Why It's Brilliant

| Aspect | Traditional Approach | nanochat Approach |
|--------|---------------------|-------------------|
| **Training** | Use HuggingFace (bloated) | Use rustbpe (simple) |
| **Inference** | Same library (okay speed) | Use tiktoken (blazing fast) |
| **Code** | One big library | Two focused tools |
| **Complexity** | 20K+ lines | 500 lines + proven lib |
| **Speed** | Good | Excellent |
| **Hackability** | Hard to modify | Easy to modify |
| **Philosophy** | Swiss Army knife | Right tool for job |

## ğŸ’¡ Key Design Insights

### 1. Separation of Concerns
```
Training:  Happens once    â†’ Optimize for simplicity & control
Inference: Happens 10^9x   â†’ Optimize for maximum speed
```

### 2. Leverage Existing Excellence
```
tiktoken = battle-tested by OpenAI on GPT-3/4
         = billions of tokens processed
         = zero bugs in production
         
Why reinvent? Just use it! ğŸ¯
```

### 3. Own What Matters
```
Training needs:
- Custom vocab size
- Custom regex pattern  
- Custom special tokens
- Integration with your pipeline

â†’ Write custom training code (rustbpe)
â†’ Keep it simple (~500 lines)
â†’ Export to standard format
```

## ğŸ”§ Code Snippets

### Training (Once)
```python
from nanochat.tokenizer import RustBPETokenizer

# Train from streaming data
tokenizer = RustBPETokenizer.train_from_iterator(
    text_iterator,
    vocab_size=50304  # 256 + 50048 merges
)

# Save
tokenizer.save("tokenizer/")
```

### Using (Always)
```python
from nanochat.tokenizer import get_tokenizer

# Load once
tokenizer = get_tokenizer()

# Encode (tiktoken internally)
ids = tokenizer.encode("Hello world", prepend="<|bos|>")
# â†’ [50304, 15496, 995]

# Decode
text = tokenizer.decode(ids)
# â†’ "<|bos|>Hello world"

# Batch encode (parallel)
ids_batch = tokenizer.encode(
    ["Hello", "world"],
    num_threads=8  # tiktoken supports this!
)
```

### Special Tokens (Chat)
```python
# nanochat defines 8 special tokens
SPECIAL_TOKENS = [
    "<|bos|>",           # Document delimiter
    "<|user_start|>",    # User: ...
    "<|user_end|>",
    "<|assistant_start|>",  # Assistant: ...
    "<|assistant_end|>",
    "<|python_start|>",  # Tool use
    "<|python_end|>",
    "<|output_start|>",  # Tool output
    "<|output_end|>",
]

# Example conversation
tokens = [
    bos,
    user_start, *encode("What is 2+2?"), user_end,
    assistant_start, *encode("Let me calculate: "),
    python_start, *encode("2+2"), python_end,
    output_start, *encode("4"), output_end,
    *encode(" The answer is 4."),
    assistant_end,
]
```

## ğŸ“ˆ Performance Tips

### Training
```python
# âœ… Stream from iterator (no memory issues)
tokenizer.train_from_iterator(
    huge_dataset_generator(),
    vocab_size=50304,
    buffer_size=8192  # Batch size for parallel processing
)

# âŒ Don't load all data in memory
text = "".join(huge_dataset)  # OOM!
```

### Inference
```python
# âœ… Batch encoding (parallel)
ids_batch = tokenizer.encode(
    texts,
    num_threads=8  # Use all cores
)

# âœ… Reuse tokenizer object
tokenizer = get_tokenizer()  # Load once
for text in texts:
    ids = tokenizer.encode(text)  # Fast!

# âŒ Don't reload tokenizer
for text in texts:
    tokenizer = get_tokenizer()  # Slow!
    ids = tokenizer.encode(text)
```

## ğŸ“ Learning Path

1. **Understand BPE algorithm**
   - Read: minbpe (simple Python implementation)
   - File: `tests/test_rustbpe.py` (reference implementation)

2. **Study rustbpe training**
   - File: `rustbpe/src/lib.rs` (~500 lines)
   - Focus: Incremental pair counting, heap-based merging

3. **Study tiktoken inference**
   - Repo: https://github.com/openai/tiktoken
   - Focus: Efficient merge application

4. **Understand the bridge**
   - File: `nanochat/tokenizer.py`
   - Focus: How rustbpe exports to tiktoken format

## ğŸ” Debugging Tips

### Check Tokenization
```python
tokenizer = get_tokenizer()

# Encode
text = "Hello world!"
ids = tokenizer.encode(text)
print(f"IDs: {ids}")

# Decode each token
for i in ids:
    token_text = tokenizer.decode([i])
    print(f"Token {i}: {repr(token_text)}")
```

### Visualize Special Tokens
```python
ids, mask = tokenizer.render_conversation(conversation)

# Visualize (green=trained, red=not trained)
print(tokenizer.visualize_tokenization(ids, mask))
```

### Compare with Reference
```python
# Test against minbpe
from tests.test_rustbpe import RegexTokenizer

ref = RegexTokenizer()
ref.train(text, vocab_size)
ref_ids = ref.encode_ordinary(text)

my_ids = tokenizer.encode(text)

assert my_ids == ref_ids, "Tokenization mismatch!"
```

## ğŸ“¦ File Locations

```
tokenizer/
â”œâ”€â”€ tokenizer.pkl          # Pickled tiktoken.Encoding object
â””â”€â”€ token_bytes.pt         # Bytes per token (for bpb metric)

rustbpe/
â””â”€â”€ src/lib.rs             # Training code (~500 lines)

nanochat/
â””â”€â”€ tokenizer.py           # Python interface

scripts/
â””â”€â”€ tok_train.py           # Training script

tests/
â””â”€â”€ test_rustbpe.py        # Comprehensive tests
```

## ğŸ¯ Decision Tree: Which Tokenizer?

```
Do you need to train a NEW tokenizer?
â”œâ”€ NO â†’ Use pretrained tokenizer
â”‚      tiktoken.get_encoding("cl100k_base")
â”‚
â””â”€ YES â†’ Do you need GPT-style BPE?
   â”œâ”€ NO â†’ Use HuggingFace or SentencePiece
   â”‚
   â””â”€ YES â†’ Use rustbpe + tiktoken! âœ…
           
           Benefits:
           - Fast training (Rust)
           - Fast inference (tiktoken)
           - Simple code (~500 lines)
           - Full control over vocab
```

## ğŸŒŸ Bottom Line

**rustbpe + tiktoken = Best of Both Worlds**

- âœ… Train fast (rustbpe in Rust)
- âœ… Infer fast (tiktoken from OpenAI)
- âœ… Stay simple (~500 lines)
- âœ… Production-ready (battle-tested)
- âœ… Full control (custom vocab/pattern)

**Philosophy:**
> Don't build monoliths. Compose specialized tools.
> Use proven code for critical paths.
> Own the parts where you need flexibility.

---

*This is how you train a $100 ChatGPT! ğŸš€*
