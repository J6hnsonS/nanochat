# ğŸ§® Manual Parameter Calculation Guide

A **complete guide** to calculating transformer parameters by hand!

---

## ğŸ¯ The Golden Rule

For any `nn.Linear(in_features, out_features, bias=False)`:
```
Parameters = in_features Ã— out_features
```

For any `nn.Embedding(num_embeddings, embedding_dim)`:
```
Parameters = num_embeddings Ã— embedding_dim
```

**That's it!** Everything else builds on this.

---

## ğŸ“ Step-by-Step: nanochat d20 Model

### Given Information
```
depth = 20
model_dim = depth Ã— 64 = 20 Ã— 64 = 1,280
num_heads = ceil(1280 / 128) = 10
head_dim = 1280 / 10 = 128
vocab_size = 65,536 (2^16)
```

---

## 1ï¸âƒ£ Token Embedding (wte)

```python
nn.Embedding(vocab_size=65536, embedding_dim=1280)
```

**Calculation:**
```
Parameters = 65,536 Ã— 1,280
           = 83,886,080
           â‰ˆ 84M
```

**Mental shortcut:**
```
65K Ã— 1.3K â‰ˆ 84M
```

---

## 2ï¸âƒ£ Single Transformer Block

### 2a. Attention Layer

The attention has **4 weight matrices**:

#### Q (Query) Projection
```python
nn.Linear(in=1280, out=10Ã—128=1280, bias=False)
```
```
Params = 1,280 Ã— 1,280 = 1,638,400
```

#### K (Key) Projection
```python
nn.Linear(in=1280, out=10Ã—128=1280, bias=False)
```
```
Params = 1,280 Ã— 1,280 = 1,638,400
```

#### V (Value) Projection
```python
nn.Linear(in=1280, out=10Ã—128=1280, bias=False)
```
```
Params = 1,280 Ã— 1,280 = 1,638,400
```

#### Output Projection
```python
nn.Linear(in=1280, out=1280, bias=False)
```
```
Params = 1,280 Ã— 1,280 = 1,638,400
```

**Total Attention:**
```
= 4 Ã— (1,280 Ã— 1,280)
= 4 Ã— 1,638,400
= 6,553,600
â‰ˆ 6.6M per block
```

**Mental formula for attention:**
```
If using MHA (n_head = n_kv_head):
  Attention params = 4 Ã— d_modelÂ²

If using MQA (n_kv_head = 1):
  Attention params = d_model Ã— (2Ã—d_model + 2Ã—head_dim)
```

---

### 2b. MLP Layer

The MLP has **2 weight matrices** with 4Ã— expansion:

#### First Linear (expansion)
```python
nn.Linear(in=1280, out=4Ã—1280=5120, bias=False)
```
```
Params = 1,280 Ã— 5,120
       = 6,553,600
```

#### Activation (ReLUÂ²)
```
Params = 0  (activations have no parameters!)
```

#### Second Linear (projection back)
```python
nn.Linear(in=5120, out=1280, bias=False)
```
```
Params = 5,120 Ã— 1,280
       = 6,553,600
```

**Total MLP:**
```
= (1,280 Ã— 5,120) + (5,120 Ã— 1,280)
= 2 Ã— (1,280 Ã— 5,120)
= 2 Ã— 6,553,600
= 13,107,200
â‰ˆ 13.1M per block
```

**Mental formula for MLP:**
```
MLP params = 2 Ã— (d_model Ã— expansion Ã— d_model)
           = 2 Ã— d_model Ã— (expansion Ã— d_model)
           = 2 Ã— 1,280 Ã— 5,120
           
With expansion=4:
  MLP params = 8 Ã— d_modelÂ²
```

---

### 2c. RMSNorm

```python
# Functional normalization - no parameters!
F.rms_norm(x, (x.size(-1),))
```
```
Params = 0
```

**Why?** No learnable Î³ (scale) or Î² (shift) parameters.

---

### Total per Block

```
Single block = Attention + MLP + RMSNorm
             = 6,553,600 + 13,107,200 + 0
             = 19,660,800
             â‰ˆ 19.7M per block
```

**Mental formula:**
```
Block params = 4Ã—dÂ² + 8Ã—dÂ² = 12Ã—dÂ²
             = 12 Ã— 1,280Â²
             = 12 Ã— 1,638,400
             = 19,660,800
```

---

## 3ï¸âƒ£ All Transformer Layers

```
Total layers = 20 blocks Ã— 19,660,800 params/block
             = 393,216,000
             â‰ˆ 393M
```

**Mental shortcut:**
```
20 Ã— 20M â‰ˆ 400M
```

---

## 4ï¸âƒ£ LM Head (Unembedding)

```python
nn.Linear(in=1280, out=65536, bias=False)
```

```
Params = 1,280 Ã— 65,536
       = 83,886,080
       â‰ˆ 84M
```

**Note:** This is the SAME size as the token embedding!
```
wte params = vocab Ã— d_model
lm_head params = d_model Ã— vocab
```
They're transposes of each other (but untied = separate weights).

---

## 5ï¸âƒ£ Total Model

```
TOTAL = Token Embedding + All Layers + LM Head
      = 83,886,080 + 393,216,000 + 83,886,080
      = 560,988,160
      â‰ˆ 561M parameters
      â‰ˆ 0.56B parameters
```

---

## ğŸ§  Mental Math Shortcuts

### Quick Approximation Formula
```
For d_model = D, num_layers = L, vocab_size = V:

Total â‰ˆ 2Ã—(VÃ—D) + LÃ—(12Ã—DÂ²)
      â‰ˆ embeddings + layers

For d20:
  â‰ˆ 2Ã—(65KÃ—1.3K) + 20Ã—(12Ã—1.3KÂ²)
  â‰ˆ 2Ã—84M + 20Ã—20M
  â‰ˆ 168M + 400M
  â‰ˆ 568M âœ“
```

### What Dominates?

**For small vocab (like 32K):**
```
Layers dominate: 12Ã—DÂ²Ã—L >> 2Ã—VÃ—D
```

**For large vocab (like 256K):**
```
Embeddings can compete with layers!
```

**For d20 (vocab=65K):**
```
Embeddings: 168M (30%)
Layers:     393M (70%)
```

---

## ğŸ“Š Component Breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding (wte)        84M  (15.0%)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Transformer Layers          393M  (70.1%)  â”‚
â”‚   â”œâ”€ Attention (per block)  6.6M           â”‚
â”‚   â”‚   â”œâ”€ Q projection       1.6M           â”‚
â”‚   â”‚   â”œâ”€ K projection       1.6M           â”‚
â”‚   â”‚   â”œâ”€ V projection       1.6M           â”‚
â”‚   â”‚   â””â”€ Output proj        1.6M           â”‚
â”‚   â”‚                                         â”‚
â”‚   â””â”€ MLP (per block)       13.1M           â”‚
â”‚       â”œâ”€ Expand            6.6M            â”‚
â”‚       â””â”€ Project           6.6M            â”‚
â”‚                                             â”‚
â”‚   20 blocks Ã— 19.7M = 393M                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LM Head (unembedding)        84M  (15.0%)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL                       561M           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¢ Compare Different Sizes

| Model | Layers | d_model | Parameters | Formula |
|-------|--------|---------|------------|---------|
| d4    | 4      | 256     | ~36M       | 2Ã—(65KÃ—256) + 4Ã—(12Ã—256Â²) |
| d12   | 12     | 768     | ~203M      | 2Ã—(65KÃ—768) + 12Ã—(12Ã—768Â²) |
| d20   | 20     | 1,280   | **561M**   | 2Ã—(65KÃ—1280) + 20Ã—(12Ã—1280Â²) |
| d26   | 26     | 1,664   | ~1.05B     | 2Ã—(65KÃ—1664) + 26Ã—(12Ã—1664Â²) |
| d32   | 32     | 2,048   | ~1.9B      | 2Ã—(65KÃ—2048) + 32Ã—(12Ã—2048Â²) |

---

## ğŸ“ Practice Problems

### Problem 1: d8 Model
```
depth = 8
model_dim = 8 Ã— 64 = 512
num_heads = ceil(512/128) = 4
vocab = 65,536

Calculate total parameters!
```

<details>
<summary>Solution</summary>

```
Token embedding = 65,536 Ã— 512 = 33,554,432

Per block:
  Attention = 4 Ã— 512Â² = 1,048,576
  MLP = 8 Ã— 512Â² = 2,097,152
  Total = 12 Ã— 512Â² = 3,145,728

All layers = 8 Ã— 3,145,728 = 25,165,824

LM head = 512 Ã— 65,536 = 33,554,432

TOTAL = 33.6M + 25.2M + 33.6M = 92.3M
```
</details>

---

### Problem 2: With MQA
```
Same d20, but with Multi-Query Attention:
  n_head = 10
  n_kv_head = 1  â† Only 1 set of K,V for all Q heads!

How many attention params now?
```

<details>
<summary>Solution</summary>

```
Q projection: 1,280 Ã— 1,280 = 1,638,400 (unchanged)
K projection: 1,280 Ã— 128 = 163,840 (much smaller!)
V projection: 1,280 Ã— 128 = 163,840 (much smaller!)
Output proj: 1,280 Ã— 1,280 = 1,638,400 (unchanged)

Total attention = 1,638,400 + 163,840 + 163,840 + 1,638,400
                = 3,604,480
                â‰ˆ 3.6M (was 6.6M with MHA!)

Saved ~45% of attention parameters!
```
</details>

---

## ğŸš€ Key Insights

1. **MLP is 2Ã— the attention params** (with 4Ã— expansion)
   ```
   Attention = 4Ã—dÂ²
   MLP = 8Ã—dÂ²
   ```

2. **Embeddings scale linearly with vocab**
   - Double vocab â†’ double embedding params
   - Double depth â†’ embedding params unchanged!

3. **Layers scale quadratically with d_model**
   - Double d_model â†’ 4Ã— more params per layer!
   - This is why width is expensive

4. **MQA/GQA saves inference memory, not training params much**
   - Main benefit: smaller KV cache during generation
   - Training param reduction: only ~25% of attention params

5. **No bias = ~d_model fewer params per linear layer**
   - For d20: saves ~1,280 params Ã— (4 attn + 2 mlp) Ã— 20 layers
   - = ~150K params (negligible)
   - But conceptually simpler!

---

## ğŸ’¡ Why This Matters

**For Research:**
- Understand scaling laws (compute vs parameters)
- Design architecture variants (GQA ratios, etc.)
- Estimate memory requirements

**For Implementation:**
- Debug shape mismatches
- Verify model loaded correctly
- Calculate memory usage (params Ã— bytes_per_param)

**For Optimization:**
- Know where parameters are (MLP-heavy!)
- Understand what to prune/quantize
- Calculate FLOPS from parameters

---

## ğŸ¯ Next Steps

Try calculating yourself:
1. Different depths (d=4, 8, 16, 32)
2. Different vocab sizes (32K, 128K, 256K)
3. With GQA (n_kv_head = 2, 4)
4. Different MLP expansion (2Ã—, 8Ã—)

The formula is always:
```
Total â‰ˆ 2Ã—(vocabÃ—d_model) + num_layersÃ—(4+2Ã—expansion)Ã—d_modelÂ²
```

Master this, and you'll understand any transformer architecture! ğŸ“
